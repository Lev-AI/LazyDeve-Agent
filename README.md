# ğŸ§  LazyDeve â€” Stateful Autonomous Development Agent

LazyDeve is a **working prototype** of a stateful AI development agent.

It demonstrates how an LLM can:
- operate on a real codebase,
- preserve project context across runs,
- execute changes deterministically,
- and maintain an auditable execution history.

This repository contains a functional FastAPI-based agent with persistent memory,
Git integration, and a closed-loop development workflow.

---


**LazyDeve** is not an IDE plugin and not a â€œstateless chat assistantâ€.  
It was created as an engineering experiment and evolved into a **stateful development agent** that can **plan, execute, verify, and iterate** on a real codebase â€” while preserving **project context as structured, inspectable artifacts**.
The architecture is modular by design, making it easy to adapt to different workflows and user needs.

LazyDeve is built around a simple idea:  
**the hardest part of AI-assisted development is not writing code â€” itâ€™s making the LLM execute the *exact* intent.**  
Even a small mismatch between intended behavior and what gets implemented can break logic and lead to long-lasting issues, introduce regressions, or silently corrupt architecture.

LazyDeve exists to reduce that intent â†’ implementation drift by turning development into a **closed-loop workflow** with persistent context, deterministic routing, and an auditable execution trail.

---


## ğŸ§ª Architectural Experiment

**An experiment in turning ChatGPT into a deterministic, stateful and auditable software engineer.**

Core roles:

* **GPT â€” brain**
* **LazyDeve â€” discipline**
* **Aider â€” executor**
* **Git â€” truth**
* **Memory â€” context**

---

## ğŸ§© System Diagram

```
User â†’ ChatGPT (Orchestrator) â†’ LazyDeve (State + Memory + Control) â†’ Aider (Code Executor) â†’ Git (Truth)
                                        â†‘
                               Memory (JSON / SQLite)
```


---
## ğŸ’¡ Concept

LazyDeve is designed as a **development loop**, not a chat:

**intent â†’ plan â†’ execute â†’ validate â†’ record â†’ iterate**

The agent interacts with a project through a controlled API surface (tools / actions), while maintaining a persistent project state that includes:
- what was changed,
- why it was changed,
- what happened during execution,
- and what should happen next.

Here, *context* is not just conversation history.  
It is a **structured memory layer** composed of human-readable and versionable artifacts (e.g., context snapshots, run metadata, commits), which makes the system reproducible and debuggable.

---

## âš™ï¸ What It Solves

LazyDeve targets a very specific failure mode of â€œvibe-codingâ€ workflows:

### Intent â†’ Implementation Drift
In many AI coding sessions, the LLM produces code that is *close* to the request, but subtly wrong:
- wrong file or wrong layer,
- inconsistent naming and structure,
- partial refactors that break interfaces,
- changes that ignore prior decisions or context,
- regressions because validation is out-of-band.

LazyDeve addresses this by enforcing:
- **persistent project state** (context is carried forward as artifacts, not vibes)
- **deterministic routing for commands** (CPL-style intent parsing to avoid ambiguous execution)
- **traceable execution** (actions/runs/changes are recorded and can be inspected)
- **iteration with memory** (the agent can reference what it did, what failed, and what was accepted)

This makes the assistant behave less like a prompt-completion engine and more like a **repeatable engineering process**.

---

## ğŸ§© What Makes LazyDeve Different

LazyDeve combines four practical properties that typical assistants donâ€™t provide together:

- **Stateful Context Engine**  
  Project context is persisted across sessions as structured artifacts (JSON + SQLite indexing), enabling retrieval, inspection, and evolution over time.

- **Action-Centric Memory**  
  The system records *what the agent actually did* (runs, outcomes, changes), not only what was discussed.

- **Closed-Loop Development Workflow**  
  The architecture is built for iterative development cycles (execute â†’ validate â†’ adjust), with explicit boundaries between reasoning, execution, and memory.

- **Transparency by Design**  
  Context artifacts are human-readable, versionable, and suitable for debugging and collaboration.

---

## ğŸ§± Why It Exists

LazyDeve started as a personal attempt to build a helper that would follow my intent precisely during iterative development.  
The project evolved into a working prototype that demonstrates an architecture where an LLM-driven agent can operate with **persistent state, controlled execution, and an auditable history of decisions and actions**.

---

## ğŸš€ Why Explore LazyDeve

- A **functional prototype** of a stateful development agent (not just a concept)
- A practical demonstration of **LLM-driven development with memory and traceability**
- A showcase of systems thinking: **orchestration + execution + context engineering**
- A foundation for planned extensions (Knowledge/Vector Memory layer, MCP orchestration)


---

## ğŸ—ï¸ Architecture Overview

For detailed architecture diagrams, internal flows, and design notes,
see the GitHub_full_architecture_and_future_modules/ directory.

### ğŸ”„ Full Data Flow Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           EXTERNAL CLIENT                                   â”‚
â”‚                    (Custom ChatGPT / REST Client / Scripts)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ HTTP REST
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LazyDeve Agent (FastAPI)                            â”‚
â”‚                              Port 8001                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /execute          â†’ AI task execution (Aider integration)                  â”‚
â”‚  /projects/*       â†’ Project management API                                 â”‚
â”‚  /api/v1/context/* â†’ Unified context endpoints                              â”‚
â”‚  /git/*            â†’ Git operations (commit, push, pull, status)            â”‚
â”‚  /openapi.yaml     â†’ OpenAPI schema for ChatGPT Apps                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      EXECUTION LAYER      â”‚      â”‚           MEMORY/CONTEXT LAYER            â”‚
â”‚                           â”‚      â”‚                                           â”‚
â”‚  â”œâ”€â”€ Aider (AI coding)    â”‚      â”‚  â”œâ”€â”€ memory.json    (actions, semantic)   â”‚
â”‚  â”œâ”€â”€ Git operations       â”‚      â”‚  â”œâ”€â”€ context_full.json (unified context)  â”‚
â”‚  â””â”€â”€ File management      â”‚      â”‚  â”œâ”€â”€ context.db    (SQLite index)         â”‚
â”‚                           â”‚      â”‚  â””â”€â”€ run_*.json    (execution logs)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                              â”‚
                    â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       GitHub Remote       â”‚      â”‚         ChatGPT Context Injection         â”‚
â”‚                           â”‚      â”‚                                           â”‚
â”‚  â”œâ”€â”€ Auto-commit          â”‚      â”‚  GET /projects/active                     â”‚
â”‚  â”œâ”€â”€ Auto-push            â”‚      â”‚  â†’ Returns full unified context           â”‚
â”‚  â””â”€â”€ Per-project repos    â”‚      â”‚  â†’ Injected into ChatGPT on init          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step-by-step flow:**

1. **Client Request**: ChatGPT (or any client) sends a task via `POST /execute`
2. **Task Processing**: Agent validates request, selects optimal LLM model
3. **AI Execution**: Aider executes the task using the selected LLM
4. **Memory Update**: Action logged to `memory.json["actions"]`
5. **Context Generation**: `generate_full_context()` creates unified `context_full.json`
6. **SQLite Indexing**: Context indexed to `context.db` for fast queries
7. **Git Operations**: Changes committed and pushed to GitHub
8. **Response**: Results returned to client with execution details

---

### Core System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LazyDeve Agent                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ORCHESTRATION LAYER (api/routes/)                                          â”‚
â”‚  â”œâ”€â”€ execute.py       â†’ Main AI task execution with Aider                   â”‚
â”‚  â”œâ”€â”€ projects.py      â†’ Project lifecycle management                        â”‚
â”‚  â”œâ”€â”€ context.py       â†’ Unified context endpoints                           â”‚
â”‚  â”œâ”€â”€ git.py           â†’ Git operations (commit, push, pull, diff)           â”‚
â”‚  â”œâ”€â”€ memory.py        â†’ Memory management endpoints                         â”‚
â”‚  â”œâ”€â”€ system.py        â†’ Health checks, OpenAPI schema                       â”‚
â”‚  â”œâ”€â”€ files.py         â†’ File read/write operations                          â”‚
â”‚  â”œâ”€â”€ analysis.py      â†’ Code analysis endpoints                             â”‚
â”‚  â”œâ”€â”€ run_local.py     â†’ Local script execution                              â”‚
â”‚  â””â”€â”€ llm.py           â†’ LLM provider switching                              â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  EXECUTION LAYER (core/)                                                    â”‚
â”‚  â”œâ”€â”€ basic_functional.py  â†’ Aider integration, subprocess management        â”‚
â”‚  â”œâ”€â”€ llm_selector.py      â†’ Multi-LLM provider selection and context-aware  â”‚
â”‚  â”œâ”€â”€ command_parser.py    â†’ Command Precision Layer (CPL) for routing       â”‚
â”‚  â””â”€â”€ event_bus.py         â†’ Event-driven hooks and post-action triggers     â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  MEMORY/CONTEXT LAYER (core/)                                               â”‚
â”‚  â”œâ”€â”€ context_full.py      â†’ Unified context generator (context_full.json)   â”‚
â”‚  â”œâ”€â”€ context_indexer.py   â†’ SQLite indexing engine (context.db)             â”‚
â”‚  â”œâ”€â”€ memory_utils.py      â†’ Memory I/O operations (memory.json)             â”‚
â”‚  â”œâ”€â”€ memory_processor.py  â†’ Semantic analysis (tech stack, confidence)      â”‚
â”‚  â”œâ”€â”€ context_manager.py   â†’ Session context lifecycle                       â”‚
â”‚  â”œâ”€â”€ context_initializer.py â†’ Context initialization on startup/switch      â”‚
â”‚  â”œâ”€â”€ commit_tracker.py    â†’ Git commit tracking and history                 â”‚
â”‚  â””â”€â”€ readme_utils.py      â†’ README extraction and processing                â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  PROJECT LAYER (core/)                                                      â”‚
â”‚  â”œâ”€â”€ project_manager.py   â†’ Project creation, switching, archival           â”‚
â”‚  â”œâ”€â”€ file_maintenance.py  â†’ FIFO trimming for log files                     â”‚
â”‚  â”œâ”€â”€ system_protection.py â†’ File/directory protection                       â”‚
â”‚  â””â”€â”€ log_manager.py       â†’ Unified JSON-based logging                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Memory Integration Flow

The memory pipeline provides unified context for AI operations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MEMORY INTEGRATION FLOW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  SOURCES (JSON files in .lazydeve/)                                         â”‚
â”‚  â”œâ”€â”€ memory.json         â†’ Actions history, semantic_context                â”‚
â”‚  â”œâ”€â”€ commit_history.json â†’ Git commit records                               â”‚
â”‚  â”œâ”€â”€ snapshot.json       â†’ Project state snapshot                           â”‚
â”‚  â”œâ”€â”€ config.json         â†’ Project configuration, user_memory               â”‚
â”‚  â””â”€â”€ session_context.json â†’ README content, session metadata                â”‚
â”‚                                                                             â”‚
â”‚                              â†“ generate_full_context()                      â”‚
â”‚                                                                             â”‚
â”‚  UNIFIED OUTPUT                                                             â”‚
â”‚  â””â”€â”€ context_full.json   â†’ Single source of truth for ChatGPT               â”‚
â”‚      â”œâ”€â”€ project_name, description, tech_stack, keywords, confidence        â”‚
â”‚      â”œâ”€â”€ readme.preview (configurable, default: 1400 chars)                 â”‚
â”‚      â”œâ”€â”€ commits.last_commit, commits.recent[]                              â”‚
â”‚      â”œâ”€â”€ activity.total_actions, recent_actions[], error_patterns[]         â”‚
â”‚      â”œâ”€â”€ snapshot.last_run, status, pending_changes                         â”‚
â”‚      â””â”€â”€ user_memory.notes (project rules/notes, max 300 chars)             â”‚
â”‚                                                                             â”‚
â”‚                              â†“ index_context_full()                         â”‚
â”‚                                                                             â”‚
â”‚  SQLITE INDEX                                                               â”‚
â”‚  â””â”€â”€ context.db          â†’ Fast queries for RAG/MCP integration             â”‚ 
â”‚      â”œâ”€â”€ commits table   â†’ Commit history index                             â”‚
â”‚      â”œâ”€â”€ runs table      â†’ Execution metadata (no stdout/stderr)            â”‚
â”‚      â”œâ”€â”€ embeddings table â†’ Ready for RAG population (Task 9)               â”‚
â”‚      â””â”€â”€ sync_metadata   â†’ FIFO trim tracking                               â”‚
â”‚                                                                             â”‚
â”‚                              â†“ ChatGPT injection                            â”‚
â”‚                                                                             â”‚
â”‚  CONSUMPTION                                                                â”‚
â”‚  â”œâ”€â”€ GET /projects/active       â†’ Returns full unified context              â”‚
â”‚  â”œâ”€â”€ GET /api/v1/context/full/* â†’ Direct context access                     â”‚
â”‚  â””â”€â”€ POST /api/v1/context/*/user-memory â†’ Save project rules/notes          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key points:**
- **JSON is the source of truth** â€” SQLite mirrors for fast queries
- **context_full.json** is generated on: agent startup, project switch, API call
- **No caching layer** â€” context is always fresh from source files
- **FIFO trimming** â€” automatic cleanup of old log entries

---

### Project Structure

Each project has an isolated Git repository and `.lazydeve/` context folder:

```
projects/<project_name>/
â”œâ”€â”€ .lazydeve/                    # LazyDeve metadata (per-project)
â”‚   â”œâ”€â”€ memory.json               # Actions history, semantic context
â”‚   â”œâ”€â”€ context_full.json         # Unified context (generated)
â”‚   â”œâ”€â”€ context.db                # SQLite index
â”‚   â”œâ”€â”€ commit_history.json       # Git commit records
â”‚   â”œâ”€â”€ snapshot.json             # Project state snapshot
â”‚   â”œâ”€â”€ config.json               # Project configuration
â”‚   â”œâ”€â”€ session_context.json      # README content, session metadata
â”‚   â”œâ”€â”€ stats.json                # Project statistics
â”‚   â””â”€â”€ logs/                     # Execution logs
â”‚       â”œâ”€â”€ run_*.json            # Run execution details
â”‚       â”œâ”€â”€ actions.log           # Plaintext action log
â”‚       â””â”€â”€ errors.log            # Error tracking
â”œâ”€â”€ src/                          # Source code
â”œâ”€â”€ tests/                        # Test files
â””â”€â”€ README.md                     # Project documentation
```

---


## ğŸ” Security Layer

LazyDeve operates **directly on a developerâ€™s local machine**, interacting with the real filesystem and project sources. This execution model requires explicit safeguards.

Security is enforced as a **multi-layered system**, independent of LLM behavior.

**Key principles:**
- The agent is treated as a privileged but constrained actor
- All constraints are enforced by code, not prompts
- Violations are deterministically blocked and logged

**Protection layers:**
- **Core boundary protection** â€” critical system and agent files are immutable
- **Root and config integrity** â€” environment and infrastructure artifacts are protected
- **Project-scoped execution** â€” all operations are restricted to the active project tree
- **Allow-list enforcement** â€” only explicitly permitted paths and actions are allowed
- **Backup and rollback safety** â€” file mutations are preceded by automatic backups
- **Audit logging** â€” all allowed and blocked actions are recorded

This layered approach allows autonomous execution while minimizing risk during local operation.

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** with pip
- **Git** (for version control)
- **Aider** (`pip install aider-chat`)
- **LLM API Key** (at minimum, OpenAI)

### Installation

```bash
# 1. Clone repository
git clone https://github.com/Lev-AI/LazyDeve.git
cd LazyDeve

# 2. Install dependencies
pip install fastapi uvicorn python-dotenv pydantic aiohttp requests langdetect openai anthropic google-generativeai mistralai aider-chat

# 3. Create .env file
cat > .env << 'EOF'
# Required
OPENAI_API_KEY=sk-your-openai-api-key-here
API_BEARER_TOKEN=your-secure-token-here
ENABLE_AUTH=false

# Optional (for multi-LLM support)
ANTHROPIC_API_KEY=sk-ant-your-key-here
GEMINI_API_KEY=your-gemini-key-here
MISTRAL_API_KEY=your-mistral-key-here

# Server
PORT=8001
PUBLIC_AGENT_URL=http://localhost:8001
EOF

# 4. Start the agent
uvicorn agent:app --host 0.0.0.0 --port 8001
```

### Verify Installation

```bash
# Health check
curl http://localhost:8001/ping-agent

# Expected response:
# {"message": "pong", "status": "healthy", ...}
```

---

## ğŸ“± ChatGPT Apps Integration

### Setup

1. **Get OpenAPI Schema**
   ```bash
   curl http://localhost:8001/openapi.yaml > openapi.yaml
   ```

2. **Create ChatGPT App**
   - Go to ChatGPT â†’ Explore GPTs â†’ Create a GPT
   - Under "Actions", paste the OpenAPI schema
   - Set the server URL to your agent's public URL
   - Configure authentication: Bearer token with your `API_BEARER_TOKEN`

3. **Verification Checklist**
   - [ ] `GET /ping-agent` returns healthy status
   - [ ] `GET /projects/list` returns project array
   - [ ] `POST /execute` with simple task succeeds

### Base URL Configuration

| Environment | Base URL |
|-------------|----------|
| Local development | `http://localhost:8001` |
| Cloudflare tunnel | `https://your-tunnel.trycloudflare.com` |
| Production | `https://agent.yourdomain.com` |

### Authentication

All authenticated endpoints require:
```
Authorization: Bearer <your-api-token>
```

Set in `.env`:
```env
API_BEARER_TOKEN=your-secure-token-here
ENABLE_AUTH=true  # Enable for production
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | Yes | â€” | OpenAI API key |
| `API_BEARER_TOKEN` | Yes | â€” | Bearer token for authentication |
| `ENABLE_AUTH` | No | `false` | Enable authentication |
| `PORT` | No | `8001` | Server port |
| `PUBLIC_AGENT_URL` | No | `http://localhost:8001` | Public URL for OpenAPI |
| `LLM_MODE` | No | `auto` | `auto` or `manual` |
| `MANUAL_LLM` | No | `gpt-4o` | Model when `LLM_MODE=manual` |
| `ANTHROPIC_API_KEY` | No | â€” | Anthropic API key |
| `GEMINI_API_KEY` | No | â€” | Google Gemini API key |
| `MISTRAL_API_KEY` | No | â€” | Mistral API key |
| `GITHUB_TOKEN` | No | â€” | GitHub personal access token |
| `GITHUB_USER` | No | â€” | GitHub username |

### LLM Mode Selection

```env
# Auto mode (default): Model selected based on task type + project context
LLM_MODE=auto

# Manual mode: All tasks use specified model
LLM_MODE=manual
MANUAL_LLM=gpt-4o
```

---

## ğŸ“Š Key Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/ping-agent` | GET | Health check |
| `/projects/list` | GET | List all projects |
| `/projects/active` | GET | Get active project with full context |
| `/projects/set-active/{name}` | POST | Switch active project |
| `/execute` | POST | Execute AI task via Aider |
| `/api/v1/context/full/{project}` | GET | Get unified context |
| `/api/v1/context/{project}/user-memory` | POST | Save project rules/notes |
| `/git/commit` | POST | Commit changes |
| `/git/push` | POST | Push to remote |
| `/git/status` | GET | Git status |
| `/openapi.yaml` | GET | OpenAPI schema |

For complete API reference, see `GET /openapi.yaml`.

---

## ğŸ”® Roadmap / Planned

The following features are **planned but not yet implemented**:

| Feature | Status | Description |
|---------|--------|-------------|
| **RagCore Integration** | ğŸ”œ Planned | External RAG service for semantic search and knowledge retrieval |
| **MCP Server** | ğŸ”œ Planned | Model Context Protocol server for multi-agent orchestration |
| **Docker Deployment** | ğŸ”œ Planned | Containerized deployment with persistent volumes |

---

## ğŸ†˜ Troubleshooting

### Common Issues

#### Agent Won't Start
```bash
# Check if port is in use
lsof -i :8001  # Linux/Mac
netstat -ano | findstr :8001  # Windows

# Verify environment
python -c "import os; print('OPENAI_API_KEY' in os.environ)"

# Check logs
tail -f logs/agent.log
```

#### Missing Dependencies
```bash
# Install all required packages
pip install fastapi  python-dotenv pydantic aiohttp requests langdetect openai anthropic google-generativeai mistralai aider-chat
```

#### Authentication Errors
```bash
# Verify token is set
echo $API_BEARER_TOKEN

# Test with token
curl -H "Authorization: Bearer your-token" http://localhost:8001/ping-agent
```

#### Git Push Failures
```bash
# Check GitHub token
curl -H "Authorization: token $GITHUB_TOKEN" https://api.github.com/user

# Verify remote
cd projects/your-project && git remote -v
```

### Getting Help

- **GitHub Issues**: Report bugs and feature requests
- **API Reference**: `GET /openapi.yaml` for complete endpoint documentation
- **Logs**: Check `logs/agent.log` for debugging information

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**LazyDeve Agent** â€” Autonomous AI development, from anywhere. ğŸš€


## ğŸ“š Documentation

- [System Architecture Overview](docs/architecture/overview.md)
- [Memory & Context Layer](docs/architecture/memory-layer.md)
- [Security Model](docs/architecture/security-model.md)
- [MCP Orchestration Roadmap](docs/roadmap/mcp-server.md)

## ğŸ›  Setup & Usage
- [Local Setup](docs/setup/local-run.md)
- [Docker Setup (Optional)](docs/setup/docker.md)
- [Environment Variables](docs/setup/env-reference.md)

## ğŸ¥ Demos & Examples
- [Usage Examples](docs/usage/examples.md)
- [Workflow Patterns](docs/usage/workflows.md)
- [Demo Videos](docs/media/videos.md)